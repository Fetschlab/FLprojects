import numpy as np
import pandas as pd
import os
from datetime import date
from scipy.io import loadmat
from dataclasses import dataclass, field

@dataclass
class Neuron:
    spiketimes: np.ndarray
    amps: np.ndarray 

    #wfs: np.ndarray = field(repr=False, default_factory=lambda: np.zeros(shape=int, dtype=np.float64))
    #template: np.ndarray = field(repr=False, default_factory=lambda: np.zeros(shape=int, dtype=np.float64))

    temp_amp: float = field(default=0.0)

    nspks: int = field(init=False)

    unit_id: int = field(default=0)
    clus_grp: int = field(default=0)
    clus_label: str = field(default='nan')
    ch_depth: tuple[int] = field(default=(0,1),metadata={'ord':('contact number','contact_depth')})
    mm_depth: int = field(default=0,metadata={'unit':'mm'})

    grid_xy: tuple[int] = field(default=(np.nan,np.nan))
    pen: int = field(default=0)
    subject: str = 'lucio'
    area: str = 'MSTd'
    electrode: str = ''
    sessionID: str = ''
    rec_date: date = date.today().strftime("%Y%m%d")

    def __post_init__(self):
        self.nspks = len(self.spiketimes)

    # static methods?

    #def isi(self, binsize=0.01, max=0.2, plot=False): 
    # return isi hist, violations count
    #def corr(self, binsize=0.01, max=0.2, plot=False):
    #def ifr(self, binsize=0.1, plot=False)
    #def plot_waveforms(self):  
    #def wf_width(self):

    #def summary(self, binsize=0.01, max=0.2, plot=False):
    #  isi, corr, ifr, wf_width 2x2 subplots


@dataclass
class RecordingSet:
    """
    data from one recording set 
        - metadata
        - list of unit instances, one per recorded unit
        - dictionary/pandas df of task events and times
    """

    units: list = field(default_factory=list)
    events: dict = field(default_factory=dict)

    sr: float = field(default=30000.0, metadata={'unit': 'Hz'})


def build_rec_set(subject, rec_date, set=1, datapath='/Volumes/homes/fetschlab/data/'):
    """
    RecordingSet is a class instance containing all simulataneously recorded neurons 
    and associated stimuli/behavioral events

    """
    session = f'{subject}{rec_date}_{set}'
    datapath = f'{datapath}{subject}/{subject}_neuro/'

    filepath = os.path.join(datapath,f'{rec_date}/{session}/')
    # npy_files = glob.glob(f'{filepath}*.npy')

    # read in cluster groups (from manual curation)
    cgs = pd.read_csv(f'{filepath}cluster_group.tsv',sep='\t')

    # read cluster info
    clus_info = pd.read_csv(f'{filepath}cluster_info.tsv',sep='\t')


    ss = np.squeeze(np.load(f'{filepath}spike_times.npy'))
    sg = np.squeeze(np.load(f'{filepath}spike_clusters.npy'))
    st = np.squeeze(np.load(f'{filepath}spike_templates.npy'))
    sa = np.squeeze(np.load(f'{filepath}amplitudes.npy')) 

    rec_set = RecordingSet()
    spiketimes = ss / rec_set.sr

    for clus in range(len(cgs)):
        unit_id = cgs['cluster_id'][clus]
        unit_info = clus_info[clus_info['cluster_id']==unit_id].to_dict('records')[0]

        unit = Neuron(spiketimes = spiketimes[sg==unit_id],
                      amps = sa[sg==unit_id],
                      unit_id = unit_id,
                      ch_depth = (unit_info['ch'], unit_info['depth']),
                      sessionID = session)
        rec_set.units.append(unit)

    return rec_set

subject = 'lucio'
rec_date = 20220512
set = 1



def get_cluster_label(cluster_input):
    pass