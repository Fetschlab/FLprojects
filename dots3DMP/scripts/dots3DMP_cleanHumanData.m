% take a human data struct generated by PLDAPS_preprocessing and clean it up a
% bit, for further analysis

% for SfN2021 poster, using:
% human_20190626-20191231: non-RT data, n=5 (_nonRT_clean)
% human_20200203-20210922: RT data, n =  (RT_clean)

% processing steps
% 1. remove sessions/subjects (e.g. training sessions) to consolidate data
% 2. remove breakfixes, training trials etc
% 3. remove v short blocks e.g. n<30
% 4. index coh - different cohs per subj/session, standardize as 1/2 (lo/hi)
% 5. clean up headings/modalities
% 6. normalize conf ratings!
% 7. save cleaned data!

clear all; close all

conftask = 1; % 1=colorbars, 2=PDW
normalize = 1;

% specify which (previously saved) mat file to load
 
subject = 'human';
paradigm = 'dots3DMP';
RTtask = 1;

if ~RTtask,  dateRange = 20190625:20191231; % non-RT
else,        dateRange = 20200213:20211117; % RT
% else,        dateRange = 20200213:20211020; % RT

end

folder = '/Users/stevenjerjian/Desktop/FetschLab/PLDAPS_data.nosync/dataStructs/';
file = [subject '_' num2str(dateRange(1)) '-' num2str(dateRange(end)) '.mat'];
load([folder file], 'data');

data = rmfield(data,'reward');

%%
% some new useful vars
for k = 1:length(data.filename)
    data.date(k,1) = str2double(data.filename{k}(9:16));
    data.subjDate{k,:} = [data.subj{k} data.filename{k}(9:16)];
end

%% 1. manual excludes (e.g., training days)

if ~RTtask
    excludes_filename = {};
    excludes_subjDate = {'AAW20190604','AAW20190612','LLV20190717','LLV20190723','LLV20190730'...
        'IWT20190719','IWT20190726','CXD20190724','EMF20190724','EMF20190906','EMF20190909'};  %
    excludes_subj = {'XRJ'}; % remaining subjs should be AAW, CXD, EMF, IWT, LLV
    subjs = {'AAW','LLV','IWT','CXD','EMF'}; % non-RT
else
    excludes_filename = {'humanVZC20200229dots3DMP1239','KVU202109281702','DCJ202110061444','DCJ202110061450'}; % KVU 09/28 first block, DCJ 10/06 first block
    excludes_subjDate = {'FRK20200216','FRK20200223','VZC20200222','DRF20210824','DRF20210826',...
        'ABF20210819','ABF20210826','DCJ20211001','KVU20210921','KVU20210924','SBG20210929','SBG20211001','SBG20211006'};
    excludes_subj = {'NEX', 'NKT', 'TST'};
%     subjs = {'DRH','SJJ','LLV','IPQ','FRK','VZC','DRF','ABF','DCJ','KVU','SBG'}; % RT
    subjs = {'DRH','SJJ','LLV','DRF','ABF','DCJ','KVU','SBG'}; % RT
%     subjs = {'DRH','SJJ','LLV','IPQ','FRK','DRF','KVU','SBG'}; % RT

end
% subjs = {'AAW' 'LLV' 'CXD' 'DRH' 'IPQ' 'SJJ' 'VZC'}; % all 'good' data (pre and post RT)

% default/testing, including everything
% excludes_filename = {};
% excludes_subjDate = {};
% excludes_subj = {};
% subjs = unique(data.subj);

removethese = ismember(data.filename,excludes_filename) | ismember(data.subjDate,excludes_subjDate) | ismember(data.subj,excludes_subj) | ~ismember(data.subj,subjs); %#ok<NASGU>
fnames = fieldnames(data);
for F = 1:length(fnames)
    eval(['data.' fnames{F} '(removethese) = [];']);
end

% now this should reflect only good data, per spreadsheet:
blocks = unique(data.filename);


% remove invalid trials (fixation breaks (which gives nans), and obvious testing trials, signaled by very large confidence (saccade
% endpoint) values
removethese = isnan(data.choice) | isnan(data.conf) | data.conf>3 ;
fnames = fieldnames(data);
for F = 1:length(fnames)
    eval(['data.' fnames{F} '(removethese) = [];']);
end

% quick look at blocks, for when some need to be excluded
[blocks,nTrialsByBlock] = blockCounts(data.filename);

% we can be pretty sure blocks with <N trials (say, 30) are to be discarded
N = 30;
removethese = ismember(data.filename,blocks(nTrialsByBlock<N));
for F = 1:length(fnames)
    eval(['data.' fnames{F} '(removethese) = [];']);
end

% quick look at blocks again, shouldn't be any with <N trials
[blocks,nTrialsByBlock] = blockCounts(data.filename);

%% organize cohs variable

% subjects should be tested with two cohs, low and high wrt vestibular
% may be different for different subjects, and actual values are arbitrary, 
% so let's just index them as 1 (low) and 2 (high), within each block.

% some pre-RT data has 0 coh (?), and 1 or 3 cohs within a 'good' data
% block

% save original 'raw' coherences
data.cohRaw = data.coherence;
data.coherence = nan(size(data.coherence));

ucohs_byBlock = nan(numel(blocks),3);
for b=1:length(blocks)
    temp = data;
    blockTrials = strcmp(data.filename,blocks{b});
    temp.cohRaw(~blockTrials) = [];
    [ucohs,~,inds] = unique(temp.cohRaw,'sorted');
    ucohs_byBlock(b,1:length(ucohs)) = ucohs';
    
    if length(ucohs)==3 % only used once (.3,.5,.9 ... pool the two low ones)
        temp.cohRaw(temp.cohRaw<0.6) = 0.4;
        [ucohs,~,inds] = unique(temp.cohRaw,'sorted');
    elseif length(ucohs)==1 % ignore? only once, 0 was used, so should default to cohI = 1
%         continue
    end
    
    data.coherence(blockTrials) = inds;

end

%% organize headings variable
% reassign headings to indices -N...0...+N, or fixed headings, within subject block (i.e.
% arbitrary) ?

uhdgs_byBlock = nan(numel(blocks),12);
for b=1:length(blocks)
    temp = data;
    blockTrials = strcmp(data.filename,blocks{b});
    temp.heading(~blockTrials) = [];
    [uhdgs,~,inds] = unique(temp.heading,'sorted');
    uhdgs_byBlock(b,1:length(uhdgs)) = uhdgs';
    
end

% simple for now, just group by hand...

data.heading(abs(data.heading)<0.01) = 0;

hdgVals = [1 2 3];
hdgVals = [2 5 10];

data.heading(data.heading<-0.5 & data.heading>=-2) = -hdgVals(1);
data.heading(data.heading>0.5 & data.heading<=2) = hdgVals(1);

data.heading(data.heading<-2 & data.heading>-6) = -hdgVals(2);
data.heading(data.heading>2 & data.heading<6) = hdgVals(2);

data.heading(data.heading<-6 & data.heading>=-12) = -hdgVals(3);
data.heading(data.heading>6 & data.heading<=12) = hdgVals(3);

%% cull data

mods = unique(data.modality); 

% data.coherence(data.coherence<=0.5) = 0.4;
% data.coherence(data.coherence>0.5) = 0.7;
cohs = unique(data.coherence); 

% remove the rest
removethese = ~ismember(data.coherence,cohs) & data.modality~=1;
for F = 1:length(fnames)
    eval(['data.' fnames{F} '(removethese) = [];']);
end
    
% the coh assigned to vestib trials (as a placeholder) depends on which
% cohs were shown in a particular block, so we need to standardize it:
data.coherence(data.modality==1) = cohs(1);

deltas = unique(data.delta); % aka conflict angle
hdgs = unique(data.heading);
% hdgs(hdgs==0) = [];

% remove the rest
removethese = ~ismember(data.heading,hdgs);
for F = 1:length(fnames)
    eval(['data.' fnames{F} '(removethese) = [];']);
end

% final look at blocks 
[blocks,nTrialsByBlock] = blockCounts(data.filename);

%% normalize confidence ratings, *within subject*
% SJ 07-2021 normalize within L/R choices too, to ameliorate any biases
% induced by noisy eye tracker

if normalize

data_orig = data;
usubj = unique(data.subj);
for s = 1:length(usubj)
    data = data_orig;
    removethese = ~strcmp(data.subj,usubj{s});
    for F = 1:length(fnames)
        eval(['data.' fnames{F} '(removethese) = [];']);
    end    
    
    % subtract min and divide by max
    
    % Across all choices
    
%     data.conf = (data.conf - min(data.conf)) / max((data.conf - min(data.conf)));
    
    % SJ 07-2021
    % SEPARATELY FOR LEFT AND RIGHT CHOICES 
    
    % ah, but what if subjects are genuinely more confident on one side
    % that the other (this would be forcibly removed from the dataset...)
    
    Lconf = data.conf(data.choice==1);
    Rconf = data.conf(data.choice==2);

    data.conf(data.choice==1) = (Lconf - min(Lconf)) / max((Lconf - min(Lconf)));
    data.conf(data.choice==2) = (Rconf - min(Rconf)) / max((Rconf - min(Rconf)));
    

    % OR
    
    % subtract/divide by *means*
%     minOfMeans = nanmin(nanmin(nanmin(nanmin(confMean))));
%     data.conf = data.conf - minOfMeans;
%     dots3DMP_parseData
%     maxOfMeans = nanmax(nanmax(nanmax(nanmax(confMean))));
%     data.conf = data.conf / maxOfMeans;

    % OR
    
    % simply cap at 1/0
    % data.conf(data.conf>1) = 1;
    % data.conf(data.conf<0) = 0;

    % append each subj to a new data struct
    if s==1
        data_new = data;
    else
        for F = 1:length(fnames)
            eval(['data_new.' fnames{F} '(end+1:end+length(data.date)) = data.' fnames{F} ';']);
        end
    end
end
data = data_new;
clear data_new data_orig
end


%%
if ~RTtask
    save([folder file(1:end-4) '_nonRT_clean.mat'],'data')
else
    save([folder file(1:end-4) '_RT_clean_Nov2021.mat'],'data')
end
% save([file(1:end-4) '_clean.mat'],'data')
fprintf('done.\n')


%%%%%%%%%%% % helper function
function [blocks,nTrialsByBlock] = blockCounts(filenames)
blocks = unique(filenames);
nTrialsByBlock = nan(length(blocks),1);
for u = 1:length(blocks)
    nTrialsByBlock(u) = sum(ismember(filenames,blocks(u)));
end
end

